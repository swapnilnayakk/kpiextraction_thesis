{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76a3aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_instruction_tuned_command_light.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_gpt-4.1_1.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_gpt-4.1-mini.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_haiku_3.5_1.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_gemma3.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_llama4_scout.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_deepseek_r1.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_self_consistency_llama_scout.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_llama4_scout_few_shot.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_llama4_scout_cot.csv\",\n",
    "    \"C:/ProjectCodeBase/ms/kpi_extraction_results_mistral_7B.csv\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7139e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bf1f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def calculate_similarity_scores(df):\n",
    "    similarities = []\n",
    "    for _, row in df.iterrows():\n",
    "        gold_list = eval(row['Gold'])\n",
    "        pred_list = eval(row['Predicted'])\n",
    "        \n",
    "        # Combine all elements of gold and pred into strings\n",
    "        gold_str = \" \".join([\" \".join(item) for item in gold_list])\n",
    "        pred_str = \" \".join([\" \".join(item) for item in pred_list])\n",
    "        \n",
    "        # Convert strings to vectors using TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        if gold_str and pred_str:  # Check if strings are not empty\n",
    "            vectors = vectorizer.fit_transform([gold_str, pred_str])\n",
    "            similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "        else:\n",
    "            similarity = 0.0\n",
    "            \n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Add similarity scores as a new column\n",
    "    df['Cosine Similarity'] = similarities\n",
    "    return df\n",
    "\n",
    "# Read and process the file\n",
    "file_path = \"C:/ProjectCodeBase/ms/kpi_extraction_results_instruction_tuned_command_light.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = calculate_similarity_scores(df)\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e159d03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/instruction_tuned_command_light:\n",
      "Precision (Cosine Similarity > 0.65): 0.47\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.58\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/gpt-4.1_1:\n",
      "Precision (Cosine Similarity > 0.65): 0.44\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.58\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/gpt-4.1-mini:\n",
      "Precision (Cosine Similarity > 0.65): 0.23\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.26\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/haiku_3.5_1:\n",
      "Precision (Cosine Similarity > 0.65): 0.50\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.61\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/gemma3:\n",
      "Precision (Cosine Similarity > 0.65): 0.48\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.61\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/llama4_scout:\n",
      "Precision (Cosine Similarity > 0.65): 0.48\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.60\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/deepseek_r1:\n",
      "Precision (Cosine Similarity > 0.65): 0.30\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.39\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/self_consistency_llama_scout:\n",
      "Precision (Cosine Similarity > 0.65): 0.48\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.61\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/llama4_scout_few_shot:\n",
      "Precision (Cosine Similarity > 0.65): 0.47\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.59\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/llama4_scout_cot:\n",
      "Precision (Cosine Similarity > 0.65): 0.46\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.59\n",
      "\n",
      "Evaluating C:/ProjectCodeBase/ms/mistral_7B:\n",
      "Precision (Cosine Similarity > 0.65): 0.40\n",
      "Max Cosine Similarity: 1.00\n",
      "Min Cosine Similarity: 0.00\n",
      "Avg Cosine Similarity: 0.50\n"
     ]
    }
   ],
   "source": [
    "def evaluate_results(file_path):\n",
    "    # Read the CSV file\n",
    "    result_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract model name from file path\n",
    "    model_name = file_path.split('\\\\')[-1].replace('kpi_extraction_results_', '').replace('.csv', '')\n",
    "    print(f\"\\nEvaluating {model_name}:\")\n",
    "    \n",
    "    # Calculate cosine similarity metrics\n",
    "    threshold = 0.65\n",
    "    cosine_similarity_scores = result_df['Cosine Similarity'].tolist()\n",
    "    \n",
    "    true_positives = sum(1 for score in cosine_similarity_scores if score > threshold)\n",
    "    false_positives = len(result_df) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-5)\n",
    "\n",
    "    print(f\"Precision (Cosine Similarity > {threshold}): {precision:.2f}\")\n",
    "\n",
    "    # Calculate similarity statistics\n",
    "    max_similarity = max(cosine_similarity_scores)\n",
    "    min_similarity = min(cosine_similarity_scores)\n",
    "    avg_similarity = sum(cosine_similarity_scores) / len(cosine_similarity_scores)\n",
    "    print(f\"Max Cosine Similarity: {max_similarity:.2f}\")\n",
    "    print(f\"Min Cosine Similarity: {min_similarity:.2f}\")\n",
    "    print(f\"Avg Cosine Similarity: {avg_similarity:.2f}\")\n",
    "\n",
    "# Evaluate each file\n",
    "for file_path in file_paths:\n",
    "    evaluate_results(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b03902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def evaluate_partial_predictions(\n",
    "    gold: List[Tuple[str, str, str]], \n",
    "    pred: List[Tuple[str, str, str]]\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Partial match evaluation for kpi names and fixed match on value and type.\n",
    "    \"\"\"\n",
    "    tp = 0  \n",
    "    fp = 0 \n",
    "    fn = 0 \n",
    "\n",
    "    gold_set = set(gold)\n",
    "    pred_set = set(pred)\n",
    "\n",
    "    for pred_tuple in pred_set:\n",
    "        pred_kpi, pred_value, pred_type = pred_tuple\n",
    "        match_found = False\n",
    "\n",
    "        # Check for partial match in gold tuples\n",
    "        for gold_tuple in gold_set:\n",
    "            gold_kpi, gold_value, gold_type = gold_tuple\n",
    "\n",
    "            # do partial match on KPI name and exact match on value and type\n",
    "            if pred_value == gold_value and pred_type == gold_type and (pred_kpi in gold_kpi) or (gold_kpi in pred_kpi):\n",
    "                tp += 1\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            fp += 1 \n",
    "\n",
    "    # Count false negatives (gold tuples not matched by any prediction)\n",
    "    for gold_tuple in gold_set:\n",
    "        gold_kpi, gold_value, gold_type = gold_tuple\n",
    "        match_found = False\n",
    "\n",
    "        for pred_tuple in pred_set:\n",
    "            pred_kpi, pred_value, pred_type = pred_tuple\n",
    "\n",
    "            # Partial match on KPI name and exact match on value and type\n",
    "            if pred_value == gold_value and pred_type == gold_type and (pred_kpi in gold_kpi) or (gold_kpi in pred_kpi):\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            fn += 1  # False negative if no match is found\n",
    "\n",
    "    # precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp + 1e-5)\n",
    "    recall = tp / (tp + fn + 1e-5)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-5)\n",
    "\n",
    "    return precision, recall, f1, tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41781ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Positives: 225, False Positives: 99, False Negatives: 150\n",
      "Overall metrics for instruction_tuned_command_light:\n",
      "Overall Precision: 0.694\n",
      "Overall Recall: 0.600\n",
      "Overall F1 Score: 0.644\n",
      "\n",
      "True Positives: 1351, False Positives: 741, False Negatives: 699\n",
      "Overall metrics for gpt-4.1_1:\n",
      "Overall Precision: 0.646\n",
      "Overall Recall: 0.659\n",
      "Overall F1 Score: 0.652\n",
      "\n",
      "True Positives: 765, False Positives: 355, False Negatives: 1276\n",
      "Overall metrics for gpt-4.1-mini:\n",
      "Overall Precision: 0.683\n",
      "Overall Recall: 0.375\n",
      "Overall F1 Score: 0.484\n",
      "\n",
      "True Positives: 1306, False Positives: 819, False Negatives: 736\n",
      "Overall metrics for haiku_3.5_1:\n",
      "Overall Precision: 0.615\n",
      "Overall Recall: 0.640\n",
      "Overall F1 Score: 0.627\n",
      "\n",
      "True Positives: 1293, False Positives: 814, False Negatives: 746\n",
      "Overall metrics for gemma3:\n",
      "Overall Precision: 0.614\n",
      "Overall Recall: 0.634\n",
      "Overall F1 Score: 0.624\n",
      "\n",
      "True Positives: 1338, False Positives: 723, False Negatives: 707\n",
      "Overall metrics for llama4_scout:\n",
      "Overall Precision: 0.649\n",
      "Overall Recall: 0.654\n",
      "Overall F1 Score: 0.652\n",
      "\n",
      "True Positives: 702, False Positives: 695, False Negatives: 1332\n",
      "Overall metrics for deepseek_r1:\n",
      "Overall Precision: 0.503\n",
      "Overall Recall: 0.345\n",
      "Overall F1 Score: 0.409\n",
      "\n",
      "True Positives: 1352, False Positives: 823, False Negatives: 696\n",
      "Overall metrics for self_consistency_llama_scout:\n",
      "Overall Precision: 0.622\n",
      "Overall Recall: 0.660\n",
      "Overall F1 Score: 0.640\n",
      "\n",
      "True Positives: 1321, False Positives: 777, False Negatives: 742\n",
      "Overall metrics for llama4_scout_few_shot:\n",
      "Overall Precision: 0.630\n",
      "Overall Recall: 0.640\n",
      "Overall F1 Score: 0.635\n",
      "\n",
      "True Positives: 1289, False Positives: 849, False Negatives: 747\n",
      "Overall metrics for llama4_scout_cot:\n",
      "Overall Precision: 0.603\n",
      "Overall Recall: 0.633\n",
      "Overall F1 Score: 0.618\n",
      "\n",
      "True Positives: 1107, False Positives: 682, False Negatives: 884\n",
      "Overall metrics for mistral_7B:\n",
      "Overall Precision: 0.619\n",
      "Overall Recall: 0.556\n",
      "Overall F1 Score: 0.586\n"
     ]
    }
   ],
   "source": [
    "def prepare_and_evaluate_file(file_path):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get model name from file path\n",
    "    model_name = file_path.split('/')[-1].replace('kpi_extraction_results_', '').replace('.csv', '')\n",
    "    # Initialize lists to store metrics for each row\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp_r = 0 \n",
    "    fp_r = 0 \n",
    "    fn_r = 0\n",
    "    \n",
    "    # Process each row in the DataFrame individually\n",
    "    for _, row in df.iterrows():\n",
    "        gold_list = eval(row['Gold'])\n",
    "        pred_list = eval(row['Predicted'])\n",
    "        \n",
    "        # Evaluate single row\n",
    "        precision, recall, f1, tp_r, fp_r, fn_r = evaluate_partial_predictions(gold_list, pred_list)\n",
    "        tp += tp_r\n",
    "        fp += fp_r\n",
    "        fn += fn_r\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    overall_precision = tp / (tp + fp)\n",
    "    overall_recall = tp / (tp + fn)\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
    "\n",
    "    print(f\"\\nTrue Positives: {tp}, False Positives: {fp}, False Negatives: {fn}\")\n",
    "    print(f\"Overall metrics for {model_name}:\")\n",
    "    print(f\"Overall Precision: {overall_precision:.3f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.3f}\")\n",
    "    print(f\"Overall F1 Score: {overall_f1:.3f}\")\n",
    "\n",
    "# Evaluate each file\n",
    "for file_path in file_paths:\n",
    "    prepare_and_evaluate_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ddbea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def evaluate_partial_similarity_predictions(\n",
    "    gold: List[Tuple[str, str, str]], \n",
    "    pred: List[Tuple[str, str, str]]\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Partial match evaluation for kpi names and fixed match on value and type.\n",
    "    \"\"\"\n",
    "    tp = 0  \n",
    "    fp = 0 \n",
    "    fn = 0 \n",
    "\n",
    "    gold_set = set(gold)\n",
    "    pred_set = set(pred)\n",
    "\n",
    "    for pred_tuple in pred_set:\n",
    "        pred_kpi, pred_value, pred_type = pred_tuple\n",
    "        match_found = False\n",
    "\n",
    "        # Check for partial match in gold tuples\n",
    "        for gold_tuple in gold_set:\n",
    "            gold_kpi, gold_value, gold_type = gold_tuple\n",
    "            # Initialize TF-IDF vectorizer\n",
    "            vectorizer = TfidfVectorizer()\n",
    "\n",
    "            # Transform the KPI names into TF-IDF vectors\n",
    "            kpi_names = [pred_kpi, gold_kpi]\n",
    "            tfidf_matrix = vectorizer.fit_transform(kpi_names)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            # do partial match on KPI name and exact match on value and type\n",
    "            if pred_value == gold_value and pred_type == gold_type and (similarity > 0.5):\n",
    "                tp += 1\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            fp += 1 \n",
    "\n",
    "    # Count false negatives (gold tuples not matched by any prediction)\n",
    "    for gold_tuple in gold_set:\n",
    "        gold_kpi, gold_value, gold_type = gold_tuple\n",
    "        match_found = False\n",
    "\n",
    "        for pred_tuple in pred_set:\n",
    "            pred_kpi, pred_value, pred_type = pred_tuple\n",
    "\n",
    "            vectorizer = TfidfVectorizer()\n",
    "\n",
    "            # Transform the KPI names into TF-IDF vectors\n",
    "            kpi_names = [gold_kpi, pred_kpi]\n",
    "            tfidf_matrix = vectorizer.fit_transform(kpi_names)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "            # Partial match on KPI name and exact match on value and type\n",
    "            if pred_value == gold_value and pred_type == gold_type and (similarity > 0.5):\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            fn += 1  # False negative if no match is found\n",
    "\n",
    "    # precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp + 1e-5)\n",
    "    recall = tp / (tp + fn + 1e-5)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-5)\n",
    "\n",
    "    return precision, recall, f1, tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63458f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Positives: 157, False Positives: 167, False Negatives: 214\n",
      "Overall metrics for instruction_tuned_command_light:\n",
      "Overall Precision: 0.485\n",
      "Overall Recall: 0.423\n",
      "Overall F1 Score: 0.452\n",
      "\n",
      "True Positives: 815, False Positives: 1277, False Negatives: 1245\n",
      "Overall metrics for gpt-4.1_1:\n",
      "Overall Precision: 0.390\n",
      "Overall Recall: 0.396\n",
      "Overall F1 Score: 0.393\n",
      "\n",
      "True Positives: 621, False Positives: 499, False Negatives: 1440\n",
      "Overall metrics for gpt-4.1-mini:\n",
      "Overall Precision: 0.554\n",
      "Overall Recall: 0.301\n",
      "Overall F1 Score: 0.390\n",
      "\n",
      "True Positives: 883, False Positives: 1242, False Negatives: 1177\n",
      "Overall metrics for haiku_3.5_1:\n",
      "Overall Precision: 0.416\n",
      "Overall Recall: 0.429\n",
      "Overall F1 Score: 0.422\n",
      "\n",
      "True Positives: 948, False Positives: 1159, False Negatives: 1113\n",
      "Overall metrics for gemma3:\n",
      "Overall Precision: 0.450\n",
      "Overall Recall: 0.460\n",
      "Overall F1 Score: 0.455\n",
      "\n",
      "True Positives: 863, False Positives: 1198, False Negatives: 1197\n",
      "Overall metrics for llama4_scout:\n",
      "Overall Precision: 0.419\n",
      "Overall Recall: 0.419\n",
      "Overall F1 Score: 0.419\n",
      "\n",
      "True Positives: 533, False Positives: 864, False Negatives: 1527\n",
      "Overall metrics for deepseek_r1:\n",
      "Overall Precision: 0.382\n",
      "Overall Recall: 0.259\n",
      "Overall F1 Score: 0.308\n",
      "\n",
      "True Positives: 884, False Positives: 1291, False Negatives: 1179\n",
      "Overall metrics for self_consistency_llama_scout:\n",
      "Overall Precision: 0.406\n",
      "Overall Recall: 0.429\n",
      "Overall F1 Score: 0.417\n",
      "\n",
      "True Positives: 824, False Positives: 1274, False Negatives: 1245\n",
      "Overall metrics for llama4_scout_few_shot:\n",
      "Overall Precision: 0.393\n",
      "Overall Recall: 0.398\n",
      "Overall F1 Score: 0.395\n",
      "\n",
      "True Positives: 740, False Positives: 1398, False Negatives: 1320\n",
      "Overall metrics for llama4_scout_cot:\n",
      "Overall Precision: 0.346\n",
      "Overall Recall: 0.359\n",
      "Overall F1 Score: 0.353\n",
      "\n",
      "True Positives: 616, False Positives: 1173, False Negatives: 1448\n",
      "Overall metrics for mistral_7B:\n",
      "Overall Precision: 0.344\n",
      "Overall Recall: 0.298\n",
      "Overall F1 Score: 0.320\n"
     ]
    }
   ],
   "source": [
    "def prepare_and_evaluate_file(file_path):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get model name from file path\n",
    "    model_name = file_path.split('/')[-1].replace('kpi_extraction_results_', '').replace('.csv', '')\n",
    "    # Initialize lists to store metrics for each row\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # Process each row in the DataFrame individually\n",
    "    for _, row in df.iterrows():\n",
    "        gold_list = eval(row['Gold'])\n",
    "        pred_list = eval(row['Predicted'])\n",
    "        \n",
    "        # Evaluate single row\n",
    "        precision, recall, f1, tp_r, fp_r, fn_r = evaluate_partial_similarity_predictions(gold_list, pred_list)\n",
    "        tp += tp_r\n",
    "        fp += fp_r\n",
    "        fn += fn_r\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    overall_precision = tp / (tp + fp + 1e-5)\n",
    "    overall_recall = tp / (tp + fn + 1e-5)\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall + 1e-5)\n",
    "\n",
    "    print(f\"\\nTrue Positives: {tp}, False Positives: {fp}, False Negatives: {fn}\")\n",
    "    print(f\"Overall metrics for {model_name}:\")\n",
    "    print(f\"Overall Precision: {overall_precision:.3f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.3f}\")\n",
    "    print(f\"Overall F1 Score: {overall_f1:.3f}\")\n",
    "\n",
    "# Evaluate each file\n",
    "for file_path in file_paths:\n",
    "    prepare_and_evaluate_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd2480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
